version: 2.1

orbs:
  python: circleci/python@2

commands:
  run_chatgpt_api_test:
    parameters:
      inference_engine:
        type: string
      model_id:
        type: string
    steps:
      - run:
          name: Run chatgpt api integration test (<<parameters.inference_engine>>, <<parameters.model_id>>)
          command: |
            source env/bin/activate

            # Function to run a command and stream output
            run_instance() {
              local instance=$1
              local port=$2
              local api_port=$3
              local log_file="${instance}.log"

              HF_HOME="$(pwd)/.hf_cache_$instance" \
              DEBUG_DISCOVERY=7 \
              DEBUG=7 \
              python3 main.py \
                --inference-engine <<parameters.inference_engine>> \
                --node-id "$instance" \
                --listen-port $port \
                --broadcast-port $((port + 1)) \
                --chatgpt-api-port $api_port \
                --chatgpt-api-response-timeout-secs 900 \
                > "$log_file" 2>&1 &

              # Start tailing the log file
              tail -f "$log_file" &

              echo $!
            }

            # Start instances
            echo "Starting first instance..."
            PID1=$(run_instance "node1" 5678 8000)
            TAIL_PID1=$!

            echo "Starting second instance..."
            PID2=$(run_instance "node2" 5679 8001)
            TAIL_PID2=$!

            # Wait for discovery
            echo "Waiting for discovery..."
            sleep 10

            # Function to check if processes are still running
            check_processes() {
              if ! kill -0 $PID1 2>/dev/null; then
                echo "First instance (PID $PID1) died unexpectedly."
                exit 1
              fi
              if ! kill -0 $PID2 2>/dev/null; then
                echo "Second instance (PID $PID2) died unexpectedly."
                exit 1
              fi
            }

            # Check processes before proceeding
            check_processes

            echo "Sending request to first instance..."
            response_1=$(curl -s http://localhost:8000/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "<<parameters.model_id>>",
                "messages": [{"role": "user", "content": "Keep responses concise. Who was the king of pop?"}],
                "temperature": 0.7
              }')
            echo "Response 1: $response_1"

            # Check processes after first response
            check_processes

            echo "Sending request to second instance..."
            response_2=$(curl -s http://localhost:8001/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "<<parameters.model_id>>",
                "messages": [{"role": "user", "content": "Keep responses concise. Who was the king of pop?"}],
                "temperature": 0.7
              }')
            echo "Response 2: $response_2"

            # Check processes after second response
            check_processes

            # Stop both instances
            echo "Stopping instances..."
            kill $PID1 $PID2 $TAIL_PID1 $TAIL_PID2

            echo ""
            if ! echo "$response_1" | grep -q "Michael Jackson" || ! echo "$response_2" | grep -q "Michael Jackson"; then
              echo "Test failed: Response does not contain 'Michael Jackson'"
              echo "Response 1: $response_1"
              echo ""
              echo "Response 2: $response_2"
              echo "Output of first instance:"
              cat node1.log
              echo "Output of second instance:"
              cat node2.log
              exit 1
            else
              echo "Test passed: Response from both nodes contains 'Michael Jackson'"
            fi

            # Clean up
            kill $PID1 $PID2 $TAIL_PID1 $TAIL_PID2
            wait

jobs:
  unit_test:
    macos:
      xcode: "15.4.0"
    resource_class: macos.m1.medium.gen1
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run:
          name: Run tests
          command: |
            source env/bin/activate
            # set TEMPERATURE to 0 for deterministic sampling
            METAL_XCODE=1 TEMPERATURE=0 python3 -m exo.inference.test_inference_engine

  discovery_integration_test:
    macos:
      xcode: "15.4.0"
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run:
          name: Run discovery integration test
          command: |
            source env/bin/activate
            DEBUG_DISCOVERY=7 DEBUG=7 python3 main.py --node-id "node1" --listen-port 5678 --broadcast-port 5679 --chatgpt-api-port 8000 > output1.log 2>&1 &
            PID1=$!
            DEBUG_DISCOVERY=7 DEBUG=7 python3 main.py --node-id "node2" --listen-port 5679 --broadcast-port 5678 --chatgpt-api-port 8001 > output2.log 2>&1 &
            PID2=$!
            sleep 10
            kill $PID1 $PID2
            if grep -q "Connected to peer" output1.log && grep -q "Connected to peer" output2.log; then
              echo "Test passed: Both instances discovered each other"
              exit 0
            else
              echo "Test failed: Devices did not discover each other"
              echo "Output of first instance:"
              cat output1.log
              echo "Output of second instance:"
              cat output2.log
              exit 1
            fi

  chatgpt_api_integration_test_mlx:
    macos:
      xcode: "15.4.0"
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run_chatgpt_api_test:
          inference_engine: mlx
          model_id: llama-3.1-8b

  test_macos_m1:
    macos:
      xcode: "15.4.0"
    resource_class: macos.m1.large.gen1
    steps:
      - checkout
      - run: system_profiler SPHardwareDataType

  # chatgpt_api_integration_test_tinygrad:
  #   macos:
  #     xcode: "15.4.0"
  #   resource_class: macos.m1.large.gen1
  #   steps:
  #     - checkout
  #     - run:
  #         name: Set up Python
  #         command: |
  #           brew install python@3.12
  #           python3.12 -m venv env
  #           source env/bin/activate
  #     - run:
  #         name: Install dependencies
  #         command: |
  #           source env/bin/activate
  #           pip install --upgrade pip
  #           pip install .
  #     - run_chatgpt_api_test:
  #         inference_engine: tinygrad
  #         model_id: llama-3-8b

workflows:
  version: 2
  build_and_test:
    jobs:
      - hold_for_approval:
          type: approval
          filters:
            branches:
              ignore: main
      - unit_test:
          filters:
            branches:
              only: main
      - discovery_integration_test:
          filters:
            branches:
              only: main
      - chatgpt_api_integration_test_mlx:
          filters:
            branches:
              only: main
      - test_macos_m1:
          filters:
            branches:
              only: main
      - unit_test:
          requires:
            - hold_for_approval
          filters:
            branches:
              ignore: main
      - discovery_integration_test:
          requires:
            - hold_for_approval
          filters:
            branches:
              ignore: main
      - chatgpt_api_integration_test_mlx:
          requires:
            - hold_for_approval
          filters:
            branches:
              ignore: main
      - test_macos_m1:
          requires:
            - hold_for_approval
          filters:
            branches:
              ignore: main